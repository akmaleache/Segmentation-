{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Segmentation_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akmaleache/Segmentation-/blob/main/Segmentation_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92cYIeQF0bAQ"
      },
      "source": [
        "# Segmentation of Indian Traffic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goS4NYXeU6uO",
        "outputId": "36ffd114-fbc9-45a4-bff6-cdeb864b987f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gG_10XW0bAR"
      },
      "source": [
        "import math\n",
        "from PIL import Image, ImageDraw\n",
        "from PIL import ImagePath\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import path\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import zipfile "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ffIWhobVnNV"
      },
      "source": [
        "zf = zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/segmentation/data.zip\",\"r\")\n",
        "data = zf.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl2voXpM0bAW"
      },
      "source": [
        "<pre>\n",
        "1. You can download the data from this link, and extract it\n",
        "\n",
        "2. All your data will be in the folder \"data\" \n",
        "\n",
        "3. Inside the data you will be having two folders\n",
        "\n",
        "|--- data\n",
        "|-----| ---- images\n",
        "|-----| ------|----- Scene 1\n",
        "|-----| ------|--------| ----- Frame 1 (image 1)\n",
        "|-----| ------|--------| ----- Frame 2 (image 2)\n",
        "|-----| ------|--------| ----- ...\n",
        "|-----| ------|----- Scene 2\n",
        "|-----| ------|--------| ----- Frame 1 (image 1)\n",
        "|-----| ------|--------| ----- Frame 2 (image 2)\n",
        "|-----| ------|--------| ----- ...\n",
        "|-----| ------|----- .....\n",
        "|-----| ---- masks\n",
        "|-----| ------|----- Scene 1\n",
        "|-----| ------|--------| ----- json 1 (labeled objects in image 1)\n",
        "|-----| ------|--------| ----- json 2 (labeled objects in image 1)\n",
        "|-----| ------|--------| ----- ...\n",
        "|-----| ------|----- Scene 2\n",
        "|-----| ------|--------| ----- json 1 (labeled objects in image 1)\n",
        "|-----| ------|--------| ----- json 2 (labeled objects in image 1)\n",
        "|-----| ------|--------| ----- ...\n",
        "|-----| ------|----- .....\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWCQ4I4d0bAX"
      },
      "source": [
        "# Task 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4owX5YO0bAY"
      },
      "source": [
        "## 1. Get all the file name and corresponding json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Q8onrE0bAZ"
      },
      "source": [
        "def return_file_names_df(root_dir):\n",
        "    # write the code that will create a dataframe with two columns ['images', 'json']\n",
        "    # the column 'image' will have path to images\n",
        "    # the column 'json' will have path to json files\n",
        "    unique_ids = os.listdir(root_dir)\n",
        "    image_path = '/content/data/images/'\n",
        "    json_path = '/content/data/mask/'\n",
        "    images = []\n",
        "    json = []\n",
        "    for i in unique_ids:        \n",
        "        images.extend([image_path+str(i)+'/'+img for img in sorted(set(os.listdir(image_path+str(i))))])# using sorted set to maintain elements order while converting set to list.(Finxter)\n",
        "        json.extend([json_path+str(i)+'/'+json for json in sorted(set(os.listdir(json_path+str(i))))])\n",
        "\n",
        "    data_df = pd.DataFrame(list(zip(images,json)),columns=['images','json'])\n",
        "    return data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Em4n2bW10bAc",
        "scrolled": true,
        "outputId": "b1d30f37-7022-4f40-9e23-559669dad0c1"
      },
      "source": [
        "root_dir = '/content/data/images'\n",
        "data_df = return_file_names_df(root_dir)\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "      <th>json</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/data/images/298/frame0139_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0139_gtFine_polygo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/data/images/298/frame0289_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0289_gtFine_polygo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/data/images/298/frame0375_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0375_gtFine_polygo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/data/images/298/frame0555_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0555_gtFine_polygo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/data/images/298/frame1056_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame1056_gtFine_polygo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              images                                               json\n",
              "0  /content/data/images/298/frame0139_leftImg8bit...  /content/data/mask/298/frame0139_gtFine_polygo...\n",
              "1  /content/data/images/298/frame0289_leftImg8bit...  /content/data/mask/298/frame0289_gtFine_polygo...\n",
              "2  /content/data/images/298/frame0375_leftImg8bit...  /content/data/mask/298/frame0375_gtFine_polygo...\n",
              "3  /content/data/images/298/frame0555_leftImg8bit...  /content/data/mask/298/frame0555_gtFine_polygo...\n",
              "4  /content/data/images/298/frame1056_leftImg8bit...  /content/data/mask/298/frame1056_gtFine_polygo..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uof88SJW0bAf"
      },
      "source": [
        "> If you observe the dataframe, we can consider each row as single data point, where first feature is image and the second feature is corresponding json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjMprsim0bAg"
      },
      "source": [
        "def grader_1(data_df):\n",
        "    for i in data_df.values:\n",
        "        if not (path.isfile(i[0]) and path.isfile(i[1]) and i[0][21:i[0].find('_')]==i[1][19:i[1].find('_')]):\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fvgVQWF0bAj",
        "outputId": "09559225-c090-43f3-bcd3-0456f904d9a9"
      },
      "source": [
        "grader_1(data_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ijL77Y10bAl",
        "outputId": "462cdac9-6334-4cc1-a174-a242209dad94"
      },
      "source": [
        "data_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4008, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB2-yrRV0bAo"
      },
      "source": [
        "## 2. Structure of sample Json file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcqXYpEI0bAp"
      },
      "source": [
        "<img src='https://i.imgur.com/EfR5KmI.png' width=\"200\" height=\"100\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ZcwNbr0bAp"
      },
      "source": [
        "* Each File will have 3 attributes\n",
        "    * imgHeight: which tells the height of the image\n",
        "    * imgWidth: which tells the width of the image\n",
        "    * objects: it is a list of objects, each object will have multiple attributes,\n",
        "        * label: the type of the object\n",
        "        * polygon: a list of two element lists, representing the coordinates of the polygon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYrICucM0bAq"
      },
      "source": [
        "#### Compute the unique labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-OYvoA20bAr"
      },
      "source": [
        "Let's see how many unique objects are there in the json file.\n",
        "to see how to get the object from the json file please check <a href='https://www.geeksforgeeks.org/read-json-file-using-python/'>this blog </a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5su9envb0bAr"
      },
      "source": [
        "def return_unique_labels(data_df):\n",
        "    labels = []\n",
        "\n",
        "    # for each file in the column json\n",
        "    #       read and store all the objects present in that file\n",
        "    # compute the unique objects and retrun them\n",
        "    # if open any json file using any editor you will get better sense of it\n",
        "    for i in data_df['json'].values:\n",
        "        f= open(i)\n",
        "        data = json.load(f)\n",
        "        for j in data['objects']: \n",
        "            labels.append(j['label'])       \n",
        "    df_label = pd.DataFrame(labels,columns=['label']) \n",
        "    return dict(df_label['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch8hYKOS0bAu"
      },
      "source": [
        "unique_labels = return_unique_labels(data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mSHuX4Y0bAw"
      },
      "source": [
        "<img src='https://i.imgur.com/L4QH6Tp.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnbt7MSc0bAw"
      },
      "source": [
        "label_clr = {'road':10, 'parking':20, 'drivable fallback':20,'sidewalk':30,'non-drivable fallback':40,'rail track':40,\\\n",
        "                        'person':50, 'animal':50, 'rider':60, 'motorcycle':70, 'bicycle':70, 'autorickshaw':80,\\\n",
        "                        'car':80, 'truck':90, 'bus':90, 'vehicle fallback':90, 'trailer':90, 'caravan':90,\\\n",
        "                        'curb':100, 'wall':100, 'fence':110,'guard rail':110, 'billboard':120,'traffic sign':120,\\\n",
        "                        'traffic light':120, 'pole':130, 'polegroup':130, 'obs-str-bar-fallback':130,'building':140,\\\n",
        "                        'bridge':140,'tunnel':140, 'vegetation':150, 'sky':160, 'fallback background':160,'unlabeled':0,\\\n",
        "                        'out of roi':0, 'ego vehicle':170, 'ground':180,'rectification border':190,\\\n",
        "                   'train':200}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEGjQstmW6ET",
        "outputId": "ed16da9c-7463-4037-9ccf-4666ed805d23"
      },
      "source": [
        "len(label_clr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPuuN7UB0bAz",
        "scrolled": true,
        "outputId": "35af1628-314c-4ccb-a817-80a5be2cca66"
      },
      "source": [
        "def grader_2(unique_labels):\n",
        "    if (not (set(label_clr.keys())-set(unique_labels))) and len(unique_labels) == 40:\n",
        "        print(\"True\")\n",
        "    else:\n",
        "        print(\"Flase\")\n",
        "\n",
        "grader_2(unique_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xmUIzX00bA2"
      },
      "source": [
        "<pre>\n",
        "* here we have given a number for each of object types, if you see we are having 21 different set of objects\n",
        "* Note that we have multiplies each object's number with 10, that is just to make different objects look differently in the segmentation map\n",
        "* Before you pass it to the models, you might need to devide the image array /10.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzUK6FMJ0bA2"
      },
      "source": [
        "## 3. Extracting the polygons from the json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRcwMw50bA3"
      },
      "source": [
        "def get_poly(file):\n",
        "    # this function will take a file name as argument\n",
        "    \n",
        "    # it will process all the objects in that file and returns\n",
        "    \n",
        "    # label: a list of labels for all the objects label[i] will have the corresponding vertices in vertexlist[i]\n",
        "    # len(label) == number of objects in the image\n",
        "    \n",
        "    # vertexlist: it should be list of list of vertices in tuple formate \n",
        "    # ex: [[(x11,y11), (x12,y12), (x13,y13) .. (x1n,y1n)]\n",
        "    #     [(x21,y21), (x22,y12), (x23,y23) .. (x2n,y2n)]\n",
        "    #      .....\n",
        "    #     [(xm1,ym1), (xm2,ym2), (xm3,ym3) .. (xmn,ymn)]]\n",
        "    # len(vertexlist) == number of objects in the image\n",
        "    \n",
        "    # * note that label[i] and vertextlist[i] are corresponds to the same object, one represents the type of the object\n",
        "    # the other represents the location\n",
        "    \n",
        "    # width of the image\n",
        "    # height of the image\n",
        "    label = []\n",
        "    vertexlist=[]\n",
        "    f= open(file)\n",
        "    data = json.load(f)\n",
        "    width = data['imgWidth']\n",
        "    height = data['imgHeight']\n",
        "    # print(data)\n",
        "    for j in data['objects']: \n",
        "        label.append(j['label']) \n",
        "        vertexlist.append([tuple(i) for i in j['polygon']])\n",
        "    return width, height, label, vertexlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlcCswQR0bA5",
        "scrolled": true,
        "outputId": "6014524a-c4b8-4c06-e980-7f3bc1932837"
      },
      "source": [
        "def grader_3(file):\n",
        "    w, h, labels, vertexlist = get_poly(file)\n",
        "    print(len((set(labels)))==18 and len(vertexlist)==227 and w==1920 and h==1080 \\\n",
        "          and isinstance(vertexlist,list) and isinstance(vertexlist[0],list) and isinstance(vertexlist[0][0],tuple) )\n",
        "\n",
        "grader_3('/content/data/mask/201/frame0029_gtFine_polygons.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw2MH_ua0bA8"
      },
      "source": [
        "## 4. Creating Image segmentations by drawing set of polygons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLrtu1f80bA8"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "-O7l9Xfj0bA9",
        "scrolled": false,
        "outputId": "f8b0dd08-615e-494d-bb63-43d978a879c0"
      },
      "source": [
        "import math \n",
        "from PIL import Image, ImageDraw \n",
        "from PIL import ImagePath  \n",
        "side=8\n",
        "x1 = [ ((math.cos(th) + 1) *9, (math.sin(th) + 1) * 6) for th in [i * (2 * math.pi) / side for i in range(side)] ]\n",
        "x2 = [ ((math.cos(th) + 2) *9, (math.sin(th) + 3) *6) for th in [i * (2 * math.pi) / side for i in range(side)] ]\n",
        "\n",
        "img = Image.new(\"RGB\", (28,28))\n",
        "img1 = ImageDraw.Draw(img)\n",
        "# please play with the fill value\n",
        "# writing the first polygon\n",
        "img1.polygon(x1, fill =20)\n",
        "# writing the second polygon\n",
        "img1.polygon(x2, fill =30)\n",
        "\n",
        "img=np.array(img)\n",
        "# note that the filling of the values happens at the channel 1, so we are considering only the first channel here\n",
        "plt.imshow(img)\n",
        "print(img.shape)\n",
        "print(img[:,:,0]//10)\n",
        "im = Image.fromarray(img[:,:,0])\n",
        "im.save(\"test_image.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 3)\n",
            "[[0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
            " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
            " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
            " [0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
            " [0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnAzJJhq0bBB"
      },
      "source": [
        "def compute_masks(data_df):\n",
        "    # after you have computed the vertexlist plot that polygone in image like this\n",
        "    \n",
        "    # img = Image.new(\"RGB\", (w, h))\n",
        "    # img1 = ImageDraw.Draw(img)\n",
        "    # img1.polygon(vertexlist[i], fill = label_clr[label[i]])\n",
        "    \n",
        "    # after drawing all the polygons that we collected from json file, \n",
        "    # you need to store that image in the folder like this \"data/output/scene/framenumber_gtFine_polygons.png\"\n",
        "    \n",
        "    # after saving the image into disk, store the path in a list\n",
        "    # after storing all the paths, add a column to the data_df['mask'] ex: data_df['mask']= mask_paths\n",
        "\n",
        "    mask = []\n",
        "    for i in tqdm(data_df['json']):\n",
        "        w,h,lable, vertex = get_poly(i)\n",
        "        img = Image.new(\"RGB\",size = (w,h))\n",
        "        img1 = ImageDraw.Draw(img)\n",
        "        for l,v in zip(lable,vertex):\n",
        "            try:\n",
        "                img1.polygon(v, fill = label_clr[l])\n",
        "            except TypeError as e:\n",
        "                pass\n",
        "                #this error arises due to empty vertex\\\n",
        "                # in polygon, un comment below print to see how many vertex is empty in this i'th json file\n",
        "                # print(i,v) \n",
        "        #changing file type from json to png and saving it to mask column\n",
        "        img=np.array(img)\n",
        "        # print(img[:,:,0]//10)\n",
        "        im = Image.fromarray(img[:,:,0])\n",
        "        p = i.replace('mask','output')\n",
        "        png_file = p.replace('json','png')\n",
        "        try:\n",
        "            os.makedirs(png_file[:24])\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "        im.save(png_file)\n",
        "        mask.append(png_file)\n",
        "    data_df['mask'] = mask\n",
        "    return data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa8l3ienlCBA"
      },
      "source": [
        "# !rm -rf '/content/data/output'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "hkDgQM280bBI",
        "outputId": "8c818f7d-218e-4145-fa7c-c550352074d4"
      },
      "source": [
        "data_df = compute_masks(data_df)\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4008/4008 [04:17<00:00, 15.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "      <th>json</th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/data/images/298/frame0139_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0139_gtFine_polygo...</td>\n",
              "      <td>/content/data/output/298/frame0139_gtFine_poly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/data/images/298/frame0289_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0289_gtFine_polygo...</td>\n",
              "      <td>/content/data/output/298/frame0289_gtFine_poly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/data/images/298/frame0375_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0375_gtFine_polygo...</td>\n",
              "      <td>/content/data/output/298/frame0375_gtFine_poly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/data/images/298/frame0555_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame0555_gtFine_polygo...</td>\n",
              "      <td>/content/data/output/298/frame0555_gtFine_poly...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/data/images/298/frame1056_leftImg8bit...</td>\n",
              "      <td>/content/data/mask/298/frame1056_gtFine_polygo...</td>\n",
              "      <td>/content/data/output/298/frame1056_gtFine_poly...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              images  ...                                               mask\n",
              "0  /content/data/images/298/frame0139_leftImg8bit...  ...  /content/data/output/298/frame0139_gtFine_poly...\n",
              "1  /content/data/images/298/frame0289_leftImg8bit...  ...  /content/data/output/298/frame0289_gtFine_poly...\n",
              "2  /content/data/images/298/frame0375_leftImg8bit...  ...  /content/data/output/298/frame0375_gtFine_poly...\n",
              "3  /content/data/images/298/frame0555_leftImg8bit...  ...  /content/data/output/298/frame0555_gtFine_poly...\n",
              "4  /content/data/images/298/frame1056_leftImg8bit...  ...  /content/data/output/298/frame1056_gtFine_poly...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "DqQibTJE0bBO",
        "scrolled": true,
        "outputId": "258b12e9-cdf6-4aef-ff40-d5b23e48300e"
      },
      "source": [
        "def grader_3():\n",
        "    url = \"https://i.imgur.com/4XSUlHk.png\"\n",
        "    url_response = urllib.request.urlopen(url)\n",
        "    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(img_array, -1)\n",
        "    my_img = cv2.imread('data/output/201/frame0029_gtFine_polygons.png')    \n",
        "    plt.imshow(my_img)\n",
        "    print((my_img[:,:,0]==img).all())\n",
        "    print(np.unique(img))\n",
        "    print(np.unique(my_img[:,:,0]))\n",
        "    data_df.to_csv('preprocessed_data.csv', index=False)\n",
        "grader_3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "[  0  10  20  40  50  60  70  80  90 100 120 130 140 150 160]\n",
            "[  0  10  20  40  50  60  70  80  90 100 120 130 140 150 160]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9RPr0aZ0bBU"
      },
      "source": [
        "# Task 2: Applying Unet to segment the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDRjb4AD0bBV"
      },
      "source": [
        "<pre>\n",
        "* please check the paper: https://arxiv.org/abs/1505.04597\n",
        "\n",
        "* <img src='https://i.imgur.com/rD4yP7J.jpg' width=\"500\">\n",
        "\n",
        "* As a part of this assignment we won't writingt this whole architecture, rather we will be doing transfer learning\n",
        "\n",
        "* please check the library <a hreaf='https://github.com/qubvel/segmentation_models'>https://github.com/qubvel/segmentation_models</a>\n",
        "\n",
        "* You can install it like this \"pip install -U segmentation-models==0.2.1\", even in google colab you can install the    same with \"!pip install -U segmentation-models==0.2.1\" \n",
        "\n",
        "* Check the reference notebook in which we have solved one end to end case study of image forgery detection using same  unet\n",
        "\n",
        "* The number of channels in the output will depend on the number of classes in your data, since we know that we are having 21 classes, the number of channels in the output will also be 21\n",
        "\n",
        "* <strong>This is where we want you to explore, how do you featurize your created segmentation map note that the original map will be of (w, h, 1) and the output will be (w, h, 21) how will you calculate the loss</strong>, you can check the examples in segmentation github\n",
        "\n",
        "* please use the loss function that is used in the refence notebooks\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrVV7zW60bBW"
      },
      "source": [
        "### Task 2.1: Dice loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zzYwXRi0bBW"
      },
      "source": [
        "<pre>\n",
        "* Explain the Dice loss\n",
        "* 1. Write the formualtion\n",
        "    D = 2 * sum(Xi * Yi) / sum(squre(Xi) * sum(sqare(Yi))\n",
        "    dice_loss = 1 - D\n",
        "* 2. Range of the loss function = 0 to 1\n",
        "* 3. Interpretation of loss function = \n",
        "* 4. Write your understanding of the loss function, how does it helps in segmentation\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akXZdZBx0bBX"
      },
      "source": [
        "### Task 2.2: Training Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5bKBqDn0bBX"
      },
      "source": [
        "\n",
        "<pre>\n",
        "* Split the data into 80:20.\n",
        "* Train the UNET on the given dataset and plot the train and validation loss.\n",
        "* As shown in the reference notebook plot 20 images from the test data along with its segmentation map, predicted map.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AejvPWmYkvz0",
        "outputId": "a52f7480-cbac-43bd-fb73-127d20ddd2e0"
      },
      "source": [
        "!pip install tensorflow==2.2.0\n",
        "!pip install keras==2.3.1 \n",
        "!pip install -U segmentation-models==0.2.1\n",
        "\n",
        "# above versions r needed to use segementation models *****************\n",
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "# from hilbert import hilbertCurve\n",
        "import imgaug.augmenters as iaa\n",
        "import numpy as np\n",
        "# import albumentations as A\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense,Input,Conv2D,MaxPool2D,Activation,Dropout,Flatten, BatchNormalization, ReLU, Reshape #CuDNNLSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import random as rn\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 74kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (56.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1 keras-applications-1.0.8\n",
            "Collecting segmentation-models==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/bf/253c8834014a834cacf2384c72872167fb30ccae7a56c6ce46285b03245c/segmentation_models-0.2.1-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from segmentation-models==0.2.1) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.7 in /usr/local/lib/python3.7/dist-packages (from segmentation-models==0.2.1) (1.0.8)\n",
            "Collecting image-classifiers==0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/32/a1e74e03f74506d1e4b46bb2732ca5a7b18ac52a36b5e3547e63537ce74c/image_classifiers-0.2.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models==0.2.1) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->segmentation-models==0.2.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.7->segmentation-models==0.2.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.7->segmentation-models==0.2.1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.1) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.1) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->segmentation-models==0.2.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.1) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models==0.2.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->segmentation-models==0.2.1) (4.4.2)\n",
            "Installing collected packages: image-classifiers, segmentation-models\n",
            "Successfully installed image-classifiers-0.2.0 segmentation-models-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imMAfY1xmOsf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(data_df, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRUbAfwWpRPs",
        "outputId": "4a582fbe-a59f-45fc-e571-4632b81ae919"
      },
      "source": [
        "# SM_FRAMEWORK=tf.keras\n",
        "import segmentation_models as sm\n",
        "from segmentation_models import Unet\n",
        "# sm.set_framework('tf.keras')\n",
        "tf.keras.backend.set_image_data_format('channels_last')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/classification_models/resnext/__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
            "  warnings.warn('Current ResNext models are deprecated, '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gahzDfprpS2H"
      },
      "source": [
        "model = Unet('resnet34', encoder_weights='imagenet', classes=1, activation='sigmoid', input_shape=(256,256,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xc4OIpHJBK0"
      },
      "source": [
        "import imgaug.augmenters as iaa\n",
        "# For the assignment choose any 4 augumentation techniques\n",
        "# check the imgaug documentations for more augmentations\n",
        "aug2 = iaa.Fliplr(1)\n",
        "aug3 = iaa.Flipud(1)\n",
        "aug4 = iaa.Emboss(alpha=(1), strength=1)\n",
        "aug5 = iaa.DirectedEdgeDetect(alpha=(0.8), direction=(1.0))\n",
        "aug6 = iaa.Sharpen(alpha=(1.0), lightness=(1.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozd5YJzwJBK0"
      },
      "source": [
        "def visualize(**images):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        if i==1:\n",
        "            plt.imshow(image, cmap='gray', vmax=1, vmin=0)\n",
        "        else:\n",
        "            plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "# def normalize_image(mask):\n",
        "#     mask = mask/255\n",
        "#     return mask\n",
        "\n",
        "class Dataset:\n",
        "    # we will be modifying this CLASSES according to your data/problems\n",
        "    # CLASSES = ['edited','non-edited']\n",
        "    \n",
        "    # the parameters needs to changed based on your requirements\n",
        "    # here we are collecting the file_names because in our dataset, both our images and maks will have same file name\n",
        "    # ex: fil_name.jpg   file_name.mask.jpg\n",
        "    def __init__(self, data, classes):\n",
        "        self.w = 256\n",
        "        self.h = 256\n",
        "        # self.ids = file_names\n",
        "        # the paths of images\n",
        "        self.images_fps   = data['images'].tolist()\n",
        "        # the paths of segmentation images\n",
        "        self.masks_fps    = data['mask'].tolist()\n",
        "        # giving labels for each class\n",
        "        self.class_values = set(list(classes.values()))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i], cv2.IMREAD_UNCHANGED) \n",
        "        image = cv2.resize(image,(self.w,self.h),interpolation=cv2.INTER_AREA)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask  = cv2.imread(self.masks_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        mask = cv2.resize(mask,(self.w,self.h),interpolation=cv2.INTER_AREA)\n",
        "        # image_mask = normalize_image(mask)\n",
        "\n",
        "        \n",
        "        image_masks = [(mask == v) for v in self.class_values] # this line use unique color(that we speciified before as lable_clr) to seperate each class\n",
        "        image_mask = np.stack(image_masks, axis=-1).astype('float')\n",
        "   \n",
        "        # a = np.random.uniform()\n",
        "        # if a<0.2:\n",
        "        #     image = aug2.augment_image(image)\n",
        "        #     image_mask = aug2.augment_image(image_mask)\n",
        "        # elif a<0.4:\n",
        "        #     image = aug3.augment_image(image)\n",
        "        #     image_mask = aug3.augment_image(image_mask)\n",
        "        # elif a<0.6:\n",
        "        #     image = aug4.augment_image(image)\n",
        "        #     image_mask = aug4.augment_image(image_mask)\n",
        "        # elif a<0.8:\n",
        "        #     image = aug5.augment_image(image)\n",
        "        #     image_mask = image_mask\n",
        "        # else:\n",
        "        #     image = aug6.augment_image(image)\n",
        "        #     image_mask = aug6.augment_image(image_mask)\n",
        "            \n",
        "        return image, image_mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images_fps)\n",
        "    \n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(dataset))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "        \n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "        \n",
        "        return tuple(batch)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57s423dJBK1"
      },
      "source": [
        "# https://github.com/qubvel/segmentation_models\n",
        "import segmentation_models as sm\n",
        "import keras\n",
        "from segmentation_models.metrics import iou_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# from segmentation_models import Unet\n",
        "\n",
        "optim = Adam()\n",
        "\n",
        "focal_loss = sm.losses.cce_dice_loss\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss \n",
        "# or total_loss = sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "model.compile(optim, focal_loss, metrics=[iou_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7HyEG18JBK1",
        "outputId": "b79ff797-6b55-4a08-982e-70f9c7119167"
      },
      "source": [
        "# Dataset for train images\n",
        "# CLASSES = ['edited']\n",
        "train_dataset = Dataset(X_train, label_clr)\n",
        "test_dataset  = Dataset(X_test, label_clr)\n",
        "\n",
        "BATCH_SIZE=1\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=1, shuffle=True)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(train_dataloader[0][0].shape)\n",
        "assert train_dataloader[0][0].shape == (BATCH_SIZE, 256,256, 3)\n",
        "assert train_dataloader[0][1].shape == (BATCH_SIZE, 256,256, 21)\n",
        "\n",
        "# custom callback for initializing lr before training\n",
        "class init_lr(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs = {}):\n",
        "        K.set_value(self.model.optimizer.lr, 0.0001)\n",
        "\n",
        "# define callbacks for learning rate scheduling and best checkpoints saving\n",
        "callbacks = [\n",
        "    ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, \\\n",
        "                                       mode='min', monitor='val_iou_score'),\n",
        "    ReduceLROnPlateau(monitor='val_iou_score', min_lr=0.000001,patience=2),\n",
        "    init_lr()\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "rkIoyAGoJBK2",
        "outputId": "6fdd3359-09c0-4d73-92ca-ee39dc3403e1"
      },
      "source": [
        "history = model.fit_generator(train_dataloader, steps_per_epoch=int(len(train_dataloader)/4), epochs=10,\\\n",
        "                              validation_data=test_dataloader,callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "801/801 [==============================] - 160s 200ms/step - loss: 0.9279 - iou_score: 0.0440 - val_loss: 0.9209 - val_iou_score: 0.0449\n",
            "Epoch 2/10\n",
            "801/801 [==============================] - 144s 180ms/step - loss: 0.9265 - iou_score: 0.0452 - val_loss: 0.9202 - val_iou_score: 0.0453\n",
            "Epoch 3/10\n",
            "801/801 [==============================] - 146s 182ms/step - loss: 0.9265 - iou_score: 0.0455 - val_loss: 0.9201 - val_iou_score: 0.0455\n",
            "Epoch 4/10\n",
            "800/801 [============================>.] - ETA: 0s - loss: 0.9264 - iou_score: 0.0456"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-c19da361aff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    243\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 raise ValueError('Output of generator should be a tuple '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTtIHDSf0bBY"
      },
      "source": [
        "# Task 3: Training CANet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM6PTaqo0bBY",
        "outputId": "d6d44dc2-2471-4368-ecad-204a26be17ae"
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Multiply\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DIdIxO0bBb"
      },
      "source": [
        "* as a part of this assignment we will be implementing the architecture based on this paper https://arxiv.org/pdf/2002.12041.pdf\n",
        "* We will be using the custom layers concept that we used in seq-seq assignment\n",
        "* You can devide the whole architecture can be devided into two parts\n",
        "    1. Encoder\n",
        "    2. Decoder\n",
        "    <img src='https://i.imgur.com/prH3Mno.png' width=\"600\">\n",
        "* Encoder:\n",
        "    * The first step of the encoder is to create the channel maps [$C_1$, $C_2$, $C_3$, $C_4$]\n",
        "    * $C_1$ width and heigths are 4x times less than the original image\n",
        "    * $C_2$ width and heigths are 8x times less than the original image\n",
        "    * $C_3$ width and heigths are 8x times less than the original image\n",
        "    * $C_4$ width and heigths are 8x times less than the original image\n",
        "    * <i>you can reduce the dimensions by using stride parameter</i>.\n",
        "    * [$C_1$, $C_2$, $C_3$, $C_4$] are formed by applying a \"conv block\" followed by $k$ number of \"identity block\". i.e the $C_k$ feature map will single \"conv block\" followed by $k$ number of \"identity blocks\".\n",
        "    <table>\n",
        "    <tr><td><img src=\"https://i.imgur.com/R8Gdypo.png\" width=\"300\"></td>\n",
        "        <td><img src=\"https://i.imgur.com/KNunjQK.png\" width=\"250\"></td></tr>\n",
        "    </table>\n",
        "    * <strong>The conv block and identity block of $C_1$</strong>: the number filters in the covolutional layers will be $[4,4,8]$ and the number of filters in the parallel conv layer will also be $8$.\n",
        "    * <strong>The conv block and identity block of $C_2$</strong>: the number filters in the covolutional layers will be $[8,8,16]$ and the number of filters in the parallel conv layer will also be $16$.\n",
        "    * <strong>The conv block and identity block of $C_3$</strong>: the number filters in the covolutional layers will be $[16,16,32]$ and the number of filters in the parallel conv layer will also be $32$.\n",
        "    * <strong>The conv block and identity block of $C_4$</strong>: the number filters in the covolutional layers will be $[32,32,64]$ and the number of filters in the parallel conv layer will also be $64$.\n",
        "    * Here $\\oplus$ represents the elementwise sum\n",
        "    <br>\n",
        "    \n",
        "    <font color=\"red\">NOTE: these filters are of your choice, you can explore more options also</font>\n",
        "    \n",
        "    * Example: if your image is of size $(512, 512, 3)$\n",
        "        * the output after $C_1$ will be $128*128*8$\n",
        "        * the output after $C_2$ will be $64*64*16$\n",
        "        * the output after $C_3$ will be $64*64*32$\n",
        "        * the output after $C_4$ will be $64*64*64$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMdxrF6p0bBb"
      },
      "source": [
        "class convolutional_block(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel=3,  filters=[4,4,8], stride=1, name=\"conv block\"):\n",
        "        super().__init__(name=name)\n",
        "        self.F1, self.F2, self.F3 = filters\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "    def call(self, X):\n",
        "        # write the architecutre that was mentioned above\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZjGHnBf0bBd"
      },
      "source": [
        "class identity_block(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel=3,  filters=[4,4,8], name=\"identity block\"):\n",
        "        super().__init__(name=name)\n",
        "        self.F1, self.F2, self.F3 = filters\n",
        "        self.kernel = kernel\n",
        "    def call(self, X):\n",
        "        # write the architecutre that was mentioned above\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNP8W_o90bBg"
      },
      "source": [
        "* The output of the $C_4$ will be passed to $\\text{Chained Context Aggregation Module (CAM)}$\n",
        "<img src='https://i.imgur.com/Bu63AAA.png' width=\"400\">\n",
        "* The CAM module will have two operations names Context flow and Global flow\n",
        "* <strong>The Global flow</strong>: \n",
        "    * as shown in the above figure first we willl apply  <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D\">global avg pooling</a> which results in (#, 1, 1, number_of_filters) then applying <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly\">BN</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\">RELU</a>, $1*1 \\text{ Conv}$ layer sequentially which results a matrix (#, 1, 1, number_of_filters). Finally apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> / <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)\n",
        "    * If you use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> then use bilinear pooling as interpolation technique\n",
        "* <strong>The Context flow</strong>: \n",
        "    * as shown in the above figure (c) the context flow will get inputs from two modules `a. C4` `b. From the above flow` \n",
        "    * We will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate\">concatinating</a> the both inputs on the last axis.\n",
        "    * After the concatination we will be applying <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D\"> Average pooling </a> which reduces the size of feature map by $N\\times$ times\n",
        "    * In the paper it was mentioned that to apply a group convolutions, but for the assignment we will be applying the simple conv layers with kernel size $(3*3)$\n",
        "    * We are skipping the channel shuffling \n",
        "    * similarly we will be applying a simple conv layers with kernel size $(3*3)$ consider this output is X\n",
        "    * later we will get the Y=(X $\\otimes \\sigma((1\\times1)conv(relu((1\\times1)conv(X))))) \\oplus X$, here $\\oplus$ is elementwise addition and $\\otimes$ is elementwise multiplication\n",
        "    * Finally apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> / <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)\n",
        "    * If you use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> then use bilinear pooling as interpolation technique\n",
        "\n",
        "NOTE: here N times reduction and N time increments makes the input and out shape same, you can explore with the N values, you can choose N = 2 or 4\n",
        "\n",
        "* Example with N=2:\n",
        "    * Assume the C4 is of shape (64,64,64) then the shape of GF will be (64,64,32)\n",
        "    * Assume the C4 is of shape (64,64,64) and the shape of GF is (64,64,32) then the shape of CF1 will be (64,64,32)\n",
        "    * Assume the C4 is of shape (64,64,64) and the shape of CF1 is (64,64,32) then the shape of CF2 will be (64,64,32)\n",
        "    * Assume the C4 is of shape (64,64,64) and the shape of CF2 is (64,64,32) then the shape of CF3 will be (64,64,32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2tIIP730bBg"
      },
      "source": [
        "class global_flow(tf.keras.layers.Layer):\n",
        "    def __init__(self, name=\"global_flow\"):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "    def call(self, X):\n",
        "        # implement the global flow operatiom\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe938KHw0bBk"
      },
      "source": [
        "class context_flow(tf.keras.layers.Layer):    \n",
        "    def __init__(self, name=\"context_flow\"):\n",
        "        super().__init__(name=name)\n",
        "    def call(self, X):\n",
        "        # here X will a list of two elements \n",
        "        INP, FLOW = X[0], X[1] \n",
        "        # implement the context flow as mentioned in the above cell\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZyxvYZp0bBm"
      },
      "source": [
        "* As shown in the above architecture we will be having 4 context flows\n",
        "* if you have implemented correctly all the shapes of Global Flow, and 3 context flows will have the same dimension\n",
        "* the output of these 4 modules will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">added</a> to get the same output matrix\n",
        "<img src='https://i.imgur.com/Bu63AAA.png' width=\"400\">\n",
        " * The output of after the sum, will be sent to the <strong>Feature selection module $FSM$</strong>\n",
        " \n",
        "* Example:\n",
        "    * if the shapes of GF, CF1, CF2, CF3 are (64,64,32), (64,64,32), (64,64,32), (64,64,32), (64,64,32) respectivly then after the sum we will be getting (64,64,32), which will be passed to the next module.\n",
        " \n",
        "<strong>Feature selection module</strong>:\n",
        "\n",
        "* As part of the FSM we will be applying a conv layer (3,3) with the padding=\"same\" so that the output and input will have same shapes\n",
        "* Let call the output as X\n",
        "* Pass the X to global pooling which results the matrix (#, 1, 1, number_of_channels)\n",
        "* Apply $1*1$ conv layer, after the pooling\n",
        "* the output of the $1*1$ conv layer will be passed to the Batch normalization layer, followed by Sigmoid activation function.\n",
        "* we will be having the output matrix of shape (#, 1, 1, number_of_channels) lets call it 'Y'\n",
        "* <strong>we can interpret this as attention mechanisum, i.e for each channel we will having a weight</strong>\n",
        "* the dimension of X (#, w, h, k) and output above steps Y is (#, 1, 1, k) i.e we need to multiply each channel of X will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Multiply\">multiplied</a> with corresponding channel of Y\n",
        "* After creating the weighted channel map we will be doing upsampling such that it will double the height and width.\n",
        "* apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> with bilinear pooling as interpolation technique\n",
        "\n",
        "* <font color=\"red\">Example</font>:\n",
        "    * Assume the matrix shape of the input is (64,64,32) then after upsampling it will be (128,128,32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNB8srLR0bBn"
      },
      "source": [
        "class fsm(tf.keras.layers.Layer):    \n",
        "    def __init__(self, name=\"feature_selection\"):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "    def call(self, X):\n",
        "        # implement the FSM modules based on image in the above cells\n",
        "        return FSM_Conv_T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xoqx8w50bBp"
      },
      "source": [
        "* <b>Adapted Global Convolutional Network (AGCN)</b>:\n",
        "    <img src=\"https://i.imgur.com/QNB8RmV.png\" width=\"300\">\n",
        "    \n",
        "    * AGCN will get the input from the output of the \"conv block\" of $C_1$\n",
        "    \n",
        "    * In all the above layers we will be using the padding=\"same\" and stride=(1,1)\n",
        "    \n",
        "    * so that we can have the input and output matrices of same size\n",
        "    \n",
        "* <font color=\"red\">Example</font>:\n",
        "    * Assume the matrix shape of the input is (128,128,32) then the output it will be (128,128,32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL_T1Muv0bBq"
      },
      "source": [
        "class agcn(tf.keras.layers.Layer):    \n",
        "    def __init__(self, name=\"global_conv_net\"):\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "    def call(self, X):\n",
        "        # please implement the above mentioned architecture\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStu3wwZ0bBs"
      },
      "source": [
        "*     <img src='https://i.imgur.com/prH3Mno.png' width=\"600\">\n",
        "* as shown in the architecture, after we get the AGCN it will get concatinated with the FSM output\n",
        "\n",
        "* If we observe the shapes both AGCN and FSM will have same height and weight\n",
        "\n",
        "* we will be concatinating both these outputs over the last axis\n",
        "\n",
        "* The concatinated output will be passed to a conv layers with filters = number of classes in our data set and the activation function = 'relu'\n",
        "\n",
        "* we will be using padding=\"same\" which results in the same size feature map\n",
        "\n",
        "* If you observe the shape of matrix, it will be 4x times less than the original image\n",
        "\n",
        "* to make it equal to the original output shape, we will do 4x times upsampling of rows and columns\n",
        "\n",
        "* apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> with bilinear pooling as interpolation technique\n",
        "\n",
        "* Finally we will be applying sigmoid activation.\n",
        "\n",
        "* Example:\n",
        "    * Assume the matrix shape of AGCN is (128,128,32)  and FSM is (128,128,32) the concatination will make it (128, 128, 64)\n",
        "    * Applying conv layer will make it (128,128,21)\n",
        "    * Finally applying upsampling will make it (512, 512, 21)\n",
        "    * Applying sigmoid will result in the same matrix (512, 512, 21)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O16er1Nx0bBs"
      },
      "source": [
        "X_input = Input(shape=(128,128,3))\n",
        "\n",
        "# Stage 1\n",
        "X = Conv2D(64, (3, 3), name='conv1', padding=\"same\", kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "X = Activation('relu')(X)\n",
        "X = MaxPooling2D((2, 2), strides=(2, 2))(X)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTOD_qPA0bBv"
      },
      "source": [
        "* If you observe the arcitecture we are creating a feature map with 2x time less width and height\n",
        "* we have written the first stage of the code above.\n",
        "* Write the next layers by using the custom layers we have written"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UwfJ2tw0bBv",
        "scrolled": true
      },
      "source": [
        "# write the complete architecutre\n",
        "\n",
        "model = Model(inputs = X, outputs = output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Cmyohz0bBz"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model, to_file='model4.png', show_shapes=True, show_layer_names=True,\n",
        "    rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXnPVQHw0bB1"
      },
      "source": [
        "### Usefull tips:\n",
        "* use \"interpolation=cv2.INTER_NEAREST\" when you are resizing the image, so that it won't mess with the number of classes\n",
        "* keep the images in the square shape like $256*256$ or $512*512$\n",
        "* Carefull when you are converting the (W, H) output image into (W, H, Classes)\n",
        "* Even for the canet, use the segmentation model's losses and the metrics\n",
        "* The goal of this assignment is make you familier in with computer vision problems, image preprocessing, building complex architectures and implementing research papers, so that in future you will be very confident in industry\n",
        "* you can use the tensorboard logss to see how is yours model's training happening\n",
        "* use callbacks that you have implemented in previous assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHWYBsLz3pRY"
      },
      "source": [
        "### Things to keep in mind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr7X4N0v1KlO"
      },
      "source": [
        "* You need to train  above built model and plot the train and test losses.\n",
        "* Make sure there is no overfitting, you are free play with the identity blocks in C1, C2, C3, C4\n",
        "* before we apply the final sigmoid activation, you can add more conv layers or BN or dropouts etc\n",
        "* you are free to use any other optimizer or learning rate or weights init or regularizations"
      ]
    }
  ]
}